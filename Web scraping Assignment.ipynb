{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481a7458-1bf7-43cc-95fd-e1c78bd1724c",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ca007-91e9-4fa0-acf0-79f2540e601a",
   "metadata": {},
   "source": [
    "Web Scraping is the process of extracting data from websites automatically.\n",
    "It is used for collecting data not accessible through APIs, gathers large datasets efficiently,\n",
    "and supports data analysis and research.\n",
    "\n",
    "Areas:\n",
    "1)Price Comparison: Extracting product prices from different websites.\n",
    "2)Market Research: Gathering data on competitors, customer sentiment, and trends.\n",
    "3)Data Journalism: Collecting information for news stories and investigations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee4867-5b7f-4c89-8320-4ba58ba77882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032223d6-0857-413a-9882-11e6552f42f2",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0e26a-b9fe-4b72-8a4c-ed29585301fb",
   "metadata": {},
   "source": [
    "There are several methods and some of them are below -\n",
    "\n",
    "1)Manual Scraping: Copying and pasting data manually (inefficient).\n",
    "2)Regular Expressions:Pattern matching for extracting text from HTML.   \n",
    "3)HTML Parsing: Using libraries like BeautifulSoup, lxml to parse HTML structure.   \n",
    "4)DOM Parsing: Interacting with the Document Object Model (DOM) for dynamic content.\n",
    "5)Web Browsers: Using tools like Selenium to simulate user interactions.\n",
    "6)API Usage: Accessing data through official APIs (if available).\n",
    "7)Web Scraping Services: Utilizing third-party platforms for scraping.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7890f9-5d29-4b2b-abea-fca63c8ad0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b3e6936-43c7-456c-9d55-ef13041fffc5",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748fa21-acf9-4a25-8b09-2ee3196334f4",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library for parsing HTML and XML documents.\n",
    "\n",
    "   \n",
    "It is used for simplifies web scraping by creating a parse tree from HTML, allowing easy navigation and data extraction. Handles messy or malformed HTML effectively.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bacf29-ec3b-4bef-a903-eb7705e9c9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ce7527-76ec-41b3-a169-736de6691e79",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d05a01-ceff-4e8d-8af9-4f817d9ffe1a",
   "metadata": {},
   "source": [
    "While Flask is primarily a web framework for building web applications, its role in a web scraping project is indirect but crucial.\n",
    "\n",
    "1.Creating a Web Interface: Flask provides the foundation for building a user-friendly interface to:\n",
    "          -Display scraped data in a structured and visually appealing format.\n",
    "          -Allow users to interact with the data, such as filtering, sorting, or searching.\n",
    "          -Provide a platform for users to input parameters or preferences for the scraping process.\n",
    "\n",
    "2.Serving Scraped Data as an API: Flask can be used to create an API endpoint that exposes the scraped data to other                  applications. This allows for integration with other systems or building more complex applications on\n",
    "             top of the scraped data.\n",
    "\n",
    "3.Data Visualization: By combining Flask with libraries like Plotly or Matplotlib, you can create interactive                          visualizations of the scraped data, providing valuable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d60edd-58f2-4184-8279-4b64d08e152b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c356f3b9-1158-4029-9e7a-876c23b4f819",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3ce72-ff97-4a52-ba3a-51c8a95fa2c9",
   "metadata": {},
   "source": [
    "1.AWS Elastic Beanstalk\n",
    "   Purpose: Manages the deployment, scaling, and updates of web applications and services.\n",
    "   Functionality: Abstracts away the infrastructure management, allowing developers to focus on application code. Handles                   provisioning EC2 instances, load balancing, auto-scaling, and application health monitoring.\n",
    "   Benefits: Simplifies deployment process, provides automatic scaling, and offers various platform options (Python,        Java, Node.js, etc.).\n",
    "\n",
    "\n",
    "2.AWS CodePipeline\n",
    "   Purpose: Fully managed continuous integration and continuous delivery (CI/CD) service.\n",
    "   Functionality: Automates the build, test, and deployment process for application updates. Integrates with version                       control systems, build tools, and deployment targets.\n",
    "   Benefits: Accelerates software delivery, improves reliability, and enables frequent code deployments.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4945294-341b-4162-aa82-fc98b1ded1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd161966-6937-4976-8356-198dee3d3080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae68777-dd44-44fe-b2cd-083da6ca5061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb2584-ddae-4198-a3a2-3a271ea970bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4630743-628a-4eb1-8350-0642bf726533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f098248-905e-49bf-9334-2f8192d1c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ed57d-4848-4c5f-8120-65b8ad5c6b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f1873-60af-43f9-9015-5c2e45373d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1caa1-977f-45fb-bf96-cd40543088bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91063c5c-c161-4bbb-bd1e-ced6f71ea2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
